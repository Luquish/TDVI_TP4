{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import IPython\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as tt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import tqdm\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from typing import Dict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from torchaudio.sox_effects import apply_effects_tensor\n",
    "from typing import Optional\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_genres(fname):\n",
    "    parts = fname.split('/')[-1].split('.')[0]\n",
    "    return parts #' '.join(parts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, root = './genres_5sec/'):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.files =[]\n",
    "        classes = self.get_classes(self.root)\n",
    "        for c in classes:\n",
    "          self.files = self.files + [fname for fname in os.listdir(os.path.join(root,c)) if fname.endswith('.wav')]\n",
    "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        fname = self.files[i]\n",
    "        genre = parse_genres(fname)\n",
    "        fpath = os.path.join(self.root,genre, fname)\n",
    "        class_idx = self.classes.index(genre)\n",
    "        audio = torchaudio.load(fpath)[0]\n",
    "\n",
    "        return audio, class_idx\n",
    "\n",
    "    def get_classes(self, root):\n",
    "        list_files = os.listdir(root)\n",
    "        classes = []\n",
    "        for file in list_files:\n",
    "            name = '{}/{}'.format(root, file)\n",
    "            if os.path.isdir(name):\n",
    "                classes.append(file)\n",
    "        return classes\n",
    "\n",
    "# [(W−K+2P)/S]+1, where W is the input size, K is the kernel size, P is the padding, and S is the stride.\n",
    "class CELAutoenconder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CELAutoenconder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=9, stride=1, padding=4), # Input size = 110250 -> Output size = 110250 (formula: [(110250-9+2*4)/1]+1 = [(110250-9+8)/1]+1 = [110250/1]+1 = 110250+1 = 110251) -> 16x110250\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Input size = 110250 -> Output size = 110249 (formula: [(110250-2)/2]+1 = [(110248)/2]+1 = 110248/2+1 = 55124+1 = 55125) -> 16x55125\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=9, stride=1, padding=4), # Input size = 55125 -> Output size = 55125 -> 32x55125\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=9, stride=1, padding=4), # Input size = 55125 -> Output size = 55125 -> 64x55125\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Input size = 55125 -> Output size = 27562 -> 64x27562\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=9, stride=1, padding=4), # Input size = 27562 -> Output size = 27562 -> 128x27562\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=9, stride=1, padding=4), # Input size = 27562 -> Output size = 27562 -> 256x27562\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Input size = 27562 -> Output size = 13781 -> 256x13781\n",
    "\n",
    "            nn.Conv1d(256, 512, kernel_size=9, stride=1, padding=4), # Input size = 13781 -> Output size = 13781 -> 512x13781\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Input size = 13781 -> Output size = 6890 -> 512x6890\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=9, stride=2, padding=4, output_padding=1), # Input size = 6890 -> Output size = 13781 -> 256x13781\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=27562),  # Tamaño específico en lugar de scale_factor\n",
    "            \n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=9, stride=1, padding=4), # Input size = 13781 -> Output size = 27562 -> 128x27562\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=9, stride=1, padding=4), # Input size = 27562 -> Output size = 55125 -> 64x27562\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=55125),  # Tamaño específico\n",
    "            \n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=9, stride=1, padding=4), # Input size = 55125 -> Output size = 110250 -> 32x55125\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=9, stride=1, padding=4), # Input size = 55125 -> Output size = 110250 -> 16x55125\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=110250),  # Tamaño específico para match exacto\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=9, stride=1, padding=4), # Input size = 110250 -> Output size = 220500 -> 1x110250\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        if decoded.size(2) != x.size(2):\n",
    "            decoded = F.interpolate(decoded, size=x.size(2), mode='linear')\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate = 22050\n",
    "\n",
    "random_seed = 43992294\n",
    "\n",
    "torch.manual_seed(random_seed);\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else\n",
    "                    'mps' if torch.backends.mps.is_available() else\n",
    "                    'cpu')\n",
    "\n",
    "print(f\"Dispositivo usado: {device}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS disponible: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, label = dataset[1]\n",
    "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(waveform.size(),samplerate,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specgram=tt.Spectrogram()(waveform)\n",
    "print(\"shape of spectogram {}\".format(specgram.size()))\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(specgram.log2()[0,:,:].numpy(),cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waveform: {}\\n\".format(waveform))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(waveform.t().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(waveform,rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experimentos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "val_size = 100\n",
    "test_size = 100\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    valid_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "    test_dl = DataLoader(test_ds,1, num_workers=4, pin_memory=True)\n",
    "else:\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(val_ds, batch_size*2)\n",
    "    test_dl = DataLoader(test_ds,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización del modelo\n",
    "autoencoder = CELAutoenconder().to(device)\n",
    "\n",
    "# Parámetros del modelo\n",
    "parameters = [p for p in autoencoder.parameters() if p.requires_grad]\n",
    "\n",
    "# Optimizador\n",
    "optimizer = optim.Adam(parameters, lr=0.001)\n",
    "\n",
    "# Obtener un lote de datos de entrenamiento\n",
    "wav, label = next(iter(train_dl))\n",
    "print(\"Forma de los datos antes de moverlos al dispositivo:\", wav.shape)\n",
    "\n",
    "# Mover los datos al dispositivo seleccionado\n",
    "wav = wav.to(device)\n",
    "\n",
    "# Imprimir la forma de los datos\n",
    "print(\"Forma de los datos después de moverlos al dispositivo:\", wav.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "valid_losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() #importante para ir liberando memoria ram\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    # Train\n",
    "    autoencoder.train()\n",
    "    for wav, _ in train_dl:\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        wav=wav.to(device)\n",
    "\n",
    "        # Forward\n",
    "        out = autoencoder(wav)\n",
    "\n",
    "        # Slice the last 23 points\n",
    "        loss = loss_function(out, wav)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        del wav \n",
    "        del loss \n",
    "        del out \n",
    "        torch.cuda.empty_cache()  \n",
    "        gc.collect() \n",
    "\n",
    "    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, _ = next(iter(train_dl))\n",
    "\n",
    "IPython.display.Audio(wav[1],rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = autoencoder(wav.to(device)).detach()\n",
    "\n",
    "IPython.display.Audio(prediction[1].cpu(),rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots vertically stacked\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot original waveform\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(wav[0, 0, :50].cpu().numpy())\n",
    "plt.title('Original Waveform')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot reconstructed waveform \n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(prediction[0, 0, :50].cpu().numpy())\n",
    "plt.title('Reconstructed Waveform')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0\n",
    "val_acc = 0\n",
    "train_acc = 0\n",
    "\n",
    "lr = 0.0001\n",
    "best_valid_loss = float('inf')  # Para trackear el mejor modelo\n",
    "best_state_dict = None  # Para guardar el mejor estado del modelo\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "iterator = tqdm(range(num_epochs), total=num_epochs, desc=\"Epoch\")\n",
    "\n",
    "for epoch in iterator:\n",
    "    train_losses_itter = []\n",
    "\n",
    "    total = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    # Train\n",
    "    autoencoder.train()\n",
    "    for wav, _, spectogram, genre_index in train_dl:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        wav = wav.to(device)\n",
    "        genre_index = torch.as_tensor(genre_index).to(device)\n",
    "\n",
    "        # Forward\n",
    "        out = autoencoder(wav)\n",
    "        \n",
    "        loss = loss_function(out.squeeze(), wav)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses_itter.append(loss.item())\n",
    "        \n",
    "        pred = out.argmax(dim=-1).flatten()\n",
    "        train_correct += (pred == genre_index).sum().item()\n",
    "        total += len(pred)\n",
    "        train_acc = 100 * train_correct / total\n",
    "\n",
    "        del wav         #importante para ir liberando memoria ram\n",
    "        del genre_index #importante para ir liberando memoria ram\n",
    "        del loss        #importante para ir liberando memoria ram\n",
    "        del out         #importante para ir liberando memoria ram\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "        gc.collect() \n",
    "\n",
    "        train_losses.append(np.mean(train_losses_itter))\n",
    "\n",
    "    #print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, np.mean(losses)))\n",
    "\n",
    "    # Validation\n",
    "    autoencoder.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    val_losses_itter = []\n",
    "    correct =0\n",
    "    for wav, _, spectogram, genre_index in valid_dl:\n",
    "\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        out = autoencoder(wav)\n",
    "\n",
    "        loss = loss_function(out.squeeze(), genre_index)\n",
    "\n",
    "        val_losses_itter.append(loss.item())\n",
    "\n",
    "        pred = out.argmax(dim=-1).flatten()\n",
    "        \n",
    "        correct += (pred == genre_index).sum().item()\n",
    "        y_true.extend(genre_index)\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "        del wav         #importante para ir liberando memoria ram\n",
    "        del genre_index #importante para ir liberando memoria ram\n",
    "        del loss        #importante para ir liberando memoria ram\n",
    "        del out         #importante para ir liberando memoria ram\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    val_acc = 100 * correct / len(valid_dl.dataset)\n",
    "    val_loss = np.mean(val_losses_itter)\n",
    "\n",
    "    # Save autoencoder\n",
    "    train_losses.append(np.mean(train_losses))\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(val_loss.item())\n",
    "    valid_accs.append(val_acc)\n",
    "\n",
    "    # Actualizar el mejor modelo si encontramos una mejor pérdida\n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        best_state_dict = autoencoder.state_dict().copy()\n",
    "        print(f'Nuevo mejor modelo encontrado en epoch {epoch+1} con validation loss: {val_loss:.4f}')\n",
    "\n",
    "# Al finalizar el entrenamiento, guardar el mejor modelo\n",
    "print(f'Guardando el mejor modelo con validation loss: {best_valid_loss:.4f}')\n",
    "torch.save(best_state_dict, 'best_model.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluación**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "S = torch.load('best_model.ckpt')\n",
    "autoencoder.load_state_dict(S)\n",
    "\n",
    "print('loaded!')\n",
    "\n",
    "# Run evaluation\n",
    "autoencoder.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for spectogram, genre_index in test_dl:\n",
    "        spectogram = spectogram.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        out = autoencoder(spectogram)\n",
    "\n",
    "        pred = out.argmax(dim=-1).flatten()\n",
    "        # append labels and predictions\n",
    "        correct += pred.eq(genre_index).sum().item()\n",
    "        y_true.extend(genre_index)\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "accuracy = correct / len(test_dl.dataset)\n",
    "print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy: %.4f' % (epoch+1, num_epochs, val_loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
